{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable names and corresponding mean and std values\n",
    "features = ['PS', 'Z3', 'U', 'V', 'T', 'lat', 'lon', 'DSE', 'RHOI', 'NETDT', 'NM', 'UTGWSPEC', 'VTGWSPEC']\n",
    "\n",
    "directory_path = '../Demodata/Convection'\n",
    "file_path_mean = '../Demodata/Convection/mean_demo_sub.npz'\n",
    "file_path_std = '../Demodata/Convection/std_demo_sub.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_variables(directory_path, variable_names, startfile, endfile):\n",
    "    # Define the variable mapping\n",
    "    variable_mapping = {\n",
    "        'NM': 'NMBV'\n",
    "    }\n",
    "\n",
    "    # Dictionary to store data for each variable\n",
    "    variable_data = {}\n",
    "\n",
    "    # Pattern to match file names\n",
    "    pattern = re.compile(r'^newCAM_demo_sub_\\d{startfile,endfile}$')\n",
    "\n",
    "    # Iterate over each data file in the directory\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        # Check if the file starts with 'newCAM_demo_sub_'\n",
    "        if file_name.startswith('newCAM_demo_sub_'):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "            # Load data from the file\n",
    "            with nc.Dataset(file_path) as dataset:\n",
    "                # Iterate over each variable name\n",
    "                for var_name in variable_names:\n",
    "                    # Check if the variable exists in the dataset\n",
    "                    mapped_name = variable_mapping.get(var_name, var_name)\n",
    "                    if mapped_name in dataset.variables:\n",
    "                        # Read the variable data\n",
    "                        var_data = dataset[mapped_name][:]\n",
    "\n",
    "                        # Store the variable data in the dictionary\n",
    "                        variable_data[var_name] = var_data\n",
    "\n",
    "    return variable_data\n",
    "\n",
    "\n",
    "def load_mean_std(file_path_mean, file_path_std, variable_names):\n",
    "    \n",
    "    # Load mean and standard deviation files\n",
    "    mean_data = np.load(file_path_mean)\n",
    "    std_data = np.load(file_path_std)\n",
    "\n",
    "    # Define dictionaries to store mean and std for each variable\n",
    "    mean_dict = {var_name: mean_data[var_name] for var_name in variable_names}\n",
    "    std_dict = {var_name: std_data[var_name] for var_name in variable_names}\n",
    "\n",
    "    return mean_dict, std_dict\n",
    "\n",
    "\n",
    "\n",
    "def normalize_data(variable_data, mean_values, std_values):\n",
    "    \n",
    "    normalized_data = {}\n",
    "\n",
    "    # Iterate over each variable in the variable data\n",
    "    for var_name, var_data in variable_data.items():\n",
    "        # Check if variable exists in the mean and std dictionaries\n",
    "        if var_name in mean_values and var_name in std_values:\n",
    "            # Extract mean and std for the variable\n",
    "            mean = mean_values[var_name]\n",
    "            std = std_values[var_name]\n",
    "\n",
    "            # Perform normalization\n",
    "            normalized_var_data = (var_data - mean) / std\n",
    "\n",
    "            # Store normalized data\n",
    "            normalized_data[var_name] = normalized_var_data\n",
    "\n",
    "    return normalized_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: PS  Shape: (1, 4419)\n",
      "Variable: Z3  Shape: (1, 93, 4419)\n",
      "Variable: U  Shape: (1, 93, 4419)\n",
      "Variable: V  Shape: (1, 93, 4419)\n",
      "Variable: T  Shape: (1, 93, 4419)\n",
      "Variable: lat  Shape: (1, 4419)\n",
      "Variable: lon  Shape: (1, 4419)\n",
      "Variable: DSE  Shape: (1, 93, 4419)\n",
      "Variable: RHOI  Shape: (1, 94, 4419)\n",
      "Variable: NETDT  Shape: (1, 93, 4419)\n",
      "Variable: NM  Shape: (1, 93, 4419)\n",
      "Variable: UTGWSPEC  Shape: (1, 93, 4419)\n",
      "Variable: VTGWSPEC  Shape: (1, 93, 4419)\n"
     ]
    }
   ],
   "source": [
    "variable_data = load_variables(directory_path, features, 1, 5)\n",
    "# print(f'Data variables: {variable_data.keys()}')\n",
    "mean_dict, std_dict = load_mean_std(file_path_mean, file_path_std, features)\n",
    "# print(f'Mean variables: {mean_dict.keys()}')\n",
    "# print(f'Std variables: {std_dict.keys()}')\n",
    "normalized_data = normalize_data(variable_data, mean_dict, std_dict)\n",
    "# print(f'Normalised variables: {normalized_data.keys()}')\n",
    "\n",
    "\n",
    "for var_name, var_data in normalized_data.items():\n",
    "    # Get the shape of the variable data\n",
    "    var_shape = var_data.shape if isinstance(var_data, np.ndarray) else \"Not an array\"\n",
    "    print(\"Variable:\", var_name, \" Shape:\", var_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct NCol = 4419\n",
    "ilev = 93\n",
    "\n",
    "Points to be considered:\n",
    "1. Some variables are different dimensions, varying over different levels (ilev=93/94 here)\n",
    "2. These levels cause the input dimensions to become large (8 variables, each with 93 instances (i.e.varying across 93 vertical levels), and 4 variables not varying across the 93 levels.)\n",
    "3. Both Input and Ouput variables have 93 levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(variable_names, normalized_data, ilev):\n",
    "    # Determine the shape of the data\n",
    "    Ncol = normalized_data[variable_names[1]].shape[2]\n",
    "    # print(f'ilev= {ilev} and Ncol={Ncol}')\n",
    "\n",
    "    # Initialize x_train and y_train arrays\n",
    "    # Calculate dim_NN and dim_NNout\n",
    "    dim_NN = int(8 * ilev + 4) # 8 variables varying over 93 levels, 4 constant variables (lat, long, PS )\n",
    "    dim_NNout = int(2 * ilev) #(UTGWSPEC, VTGWSPEC)\n",
    "\n",
    "    # Initialize x_train and y_train arrays\n",
    "    x_train = np.zeros([dim_NN, Ncol])\n",
    "    y_train = np.zeros([dim_NNout, Ncol])\n",
    "\n",
    "    # print(f'Set xtrain shape{x_train.shape}')\n",
    "    # print(f'Set ytrain shape{y_train.shape}')\n",
    "    target_var = ['UTGWSPEC','VTGWSPEC']\n",
    "\n",
    "    # Assign variables to x_train\n",
    "    y_index = 0\n",
    "    x_index = 0\n",
    "    for var_name, var_data in normalized_data.items():\n",
    "        var_shape = var_data.shape\n",
    "\n",
    "        if var_name in target_var:\n",
    "            # print(var_name, y_index\n",
    "            y_train[y_index * ilev:(y_index + 1) * ilev, :] = var_data.reshape(ilev, Ncol)\n",
    "            y_index +=1\n",
    "         \n",
    "        elif len(var_shape) == 2:  # For 2D variables\n",
    "            #  print(var_name, x_index)\n",
    "             x_train[x_index, :] = var_data\n",
    "          \n",
    "        elif len(var_shape) == 3:\n",
    "            new_ilev = var_shape[1]\n",
    "            # print(var_name, x_index)\n",
    "            x_train[x_index:x_index + new_ilev, :] = var_data ### Issue here in extracting variables level-wise because of difference in levels\n",
    "        x_index+=1\n",
    "\n",
    "    return x_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(748, 4419) (186, 4419)\n"
     ]
    }
   ],
   "source": [
    "xtrain, ytrain = data_loader(features, normalized_data, ilev=93)\n",
    "print(xtrain.shape, ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required for feeding the data into NN.\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            X (tensor): Input data.\n",
    "            Y (tensor): Output data.\n",
    "        \"\"\"\n",
    "        self.features = torch.tensor(X, dtype=torch.float64)\n",
    "        self.labels = torch.tensor(Y, dtype=torch.float64)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Function that is called when you call len(dataloader)\"\"\"\n",
    "        return len(self.features.T)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Function that is called when you call dataloader\"\"\"\n",
    "        feature = self.features[:, idx]\n",
    "        label = self.labels[:, idx]\n",
    "\n",
    "        return feature, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = myDataset(X=xtrain, Y=ytrain)\n",
    "split_data = torch.utils.data.random_split(data, [0.75, 0.25],generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_dataloader = DataLoader(split_data[0], batch_size=128, shuffle=True)\n",
    "val_dataloader = DataLoader(split_data[1], batch_size=len(split_data[1]), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Create an instance of FullyConnected NN model.\"\"\"\n",
    "        super(FullyConnected, self).__init__()\n",
    "        ilev = 93\n",
    "        hidden_layers = 8  # Number of hidden layers\n",
    "        hidden_size = 500  # Number of neurons in each hidden layer\n",
    "\n",
    "        layers = []\n",
    "        input_size = 8 * ilev + 4\n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(nn.Linear(input_size, hidden_size, dtype=torch.float64))\n",
    "            layers.append(nn.SiLU())\n",
    "            input_size = hidden_size\n",
    "\n",
    "        layers.append(nn.Linear(hidden_size, 2 * ilev, dtype=torch.float64))\n",
    "\n",
    "        self.linear_stack = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear_stack(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        \"\"\"Create an instance of EarlyStopper class.\"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss, model=None):\n",
    "        \"\"\"\n",
    "        Check if early stopping condition is met.\n",
    "\n",
    "        Args:\n",
    "            validation_loss (float): Loss value on the validation set.\n",
    "            model (nn.Module, optional): Model to be saved if early stopping condition is met.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            bool: True if early stopping condition is met, False otherwise.\n",
    "        \"\"\"\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "\n",
    "            # Save model\n",
    "            if model is not None:\n",
    "                torch.save(model.state_dict(), 'conv_torch.pth')\n",
    "\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    avg_loss = 0\n",
    "    for batch, (X, Y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, Y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss.item()  # Accumulate loss as a float\n",
    "\n",
    "    avg_loss /= len(dataloader)\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "\n",
    "def val_loop(dataloader, model, loss_fn):\n",
    "    avg_loss = sum(loss_fn(model(X), Y).item() for X, Y in dataloader) / len(dataloader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_early_stopping(train_dataloader, val_dataloader, model, optimizer, criterion, early_stopper, epochs=100):\n",
    "    train_losses = []\n",
    "    val_losses = [0]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epoch % 2 == 0:\n",
    "            print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "            print(val_losses[-1])\n",
    "            print('counter=' + str(early_stopper.counter))\n",
    "\n",
    "        train_loss = train_loop(train_dataloader, model, criterion, optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        val_loss = val_loop(val_dataloader, model, criterion)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if early_stopper.early_stop(val_loss, model):\n",
    "            print(\"BREAK!\")\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "0\n",
      "counter=0\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "0.8489934648869807\n",
      "counter=0\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "0.848900956575377\n",
      "counter=0\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "0.8488277427356834\n",
      "counter=0\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "0.8487524616774139\n",
      "counter=0\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "0.8486892791301094\n",
      "counter=0\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "0.8486358109089807\n",
      "counter=0\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "0.8485900223018678\n",
      "counter=0\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "0.8485513591271571\n",
      "counter=0\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "0.8485374125600498\n",
      "counter=0\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "0.8485556882976334\n",
      "counter=2\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "0.8485675317451268\n",
      "counter=4\n",
      "BREAK!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-5\n",
    "epochs = 100\n",
    "\n",
    "model = FullyConnected()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "early_stopper = EarlyStopper(patience=5, min_delta=0)\n",
    "\n",
    "train_losses, val_losses = train_with_early_stopping(train_dataloader, val_dataloader, model, optimizer, criterion, early_stopper, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(input_data, model):\n",
    "    # Convert input data to tensors\n",
    "    input_tensors = {key: torch.tensor(value) for key, value in input_data.items()}\n",
    "\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Forward pass to make predictions\n",
    "    with torch.no_grad():\n",
    "        predictions = model(**input_tensors)\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_variables(directory_path, features, 5,6)\n",
    "mean_dict, std_dict = load_mean_std(file_path_mean, file_path_std, features)\n",
    "normalized_test_data = normalize_data(test_data, mean_dict, std_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'conv_torch.pth'\n",
    "model = FullyConnected()\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(748, 4419) (186, 4419)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_test, y_test = data_loader(features, normalized_test_data, ilev=93)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "test_data = myDataset(X=x_test, Y=y_test)\n",
    "\n",
    "test_loader = DataLoader(data, batch_size=len(data), shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
